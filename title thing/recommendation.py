import os
import json
import numpy as np
from dotenv import load_dotenv
from openai import OpenAI
from typing import List, Tuple, Dict
from sklearn.metrics.pairwise import cosine_similarity
import pandas as pd

# .env íŒŒì¼ì—ì„œ API í‚¤ ë¡œë“œ
load_dotenv()

# Upstage API í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
client = OpenAI(
    api_key=os.getenv("UPSTAGE_API_KEY"),
    base_url="https://api.upstage.ai/v1"
)

# ë°ì´í„° íŒŒì¼ ê²½ë¡œë¥¼ ëª¨ë“ˆ ìƒë‹¨ì—ì„œ ì •ì˜
base_dir = os.path.dirname(os.path.abspath(__file__))  # .../app/core
data_dir = os.path.abspath(os.path.join(base_dir, "..", "data"))  # .../app/data

def get_embedding(text: str) -> List[float]:
    """ë‹¨ì¼ í…ìŠ¤íŠ¸ì˜ ì„ë² ë”©ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        response = client.embeddings.create(
            model="embedding-passage",
            input=[text]
        )
        return response.data[0].embedding
    except Exception as e:
        print(f"Error getting embedding for text: {e}")
        return []

def load_saved_data() -> Tuple[Dict, Dict]:
    """ì €ì¥ëœ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤."""
    # example_sentences.json íŒŒì¼ì—ì„œ ë¬¸ì¥ë“¤ ë¶ˆëŸ¬ì˜¤ê¸°
    with open(os.path.join(data_dir, 'example_sentences.json'), 'r', encoding='utf-8') as f:
        feature_sentences = json.load(f)
    # feature_embeddings.json íŒŒì¼ì—ì„œ ì„ë² ë”© ë¶ˆëŸ¬ì˜¤ê¸°
    with open(os.path.join(data_dir, 'feature_embeddings.json'), 'r', encoding='utf-8') as f:
        feature_embeddings = json.load(f)
    return feature_sentences, feature_embeddings

def find_similar_sentences(query: str, feature_sentences: Dict, feature_embeddings: Dict, top_k: int = 50) -> List[Tuple[str, str, float]]:
    """ì¿¼ë¦¬ì™€ ìœ ì‚¬í•œ ë¬¸ì¥ë“¤ì„ ì°¾ìŠµë‹ˆë‹¤."""
    # ì¿¼ë¦¬ ì„ë² ë”©
    query_embedding = get_embedding(query)
    if not query_embedding:
        return []
    
    all_similarities = []
    
    # ê° featureë³„ë¡œ ìœ ì‚¬ë„ ê³„ì‚°
    for feature, embeddings in feature_embeddings.items():
        sentences = feature_sentences[feature]
        
        for i, embedding in enumerate(embeddings):
            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            similarity = cosine_similarity([query_embedding], [embedding])[0][0]
            all_similarities.append((feature, sentences[i], similarity))
    
    # ìœ ì‚¬ë„ê°€ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬
    all_similarities.sort(key=lambda x: x[2], reverse=True)
    
    return all_similarities[:top_k]

def calculate_feature_scores_with_examples(query: str, feature_sentences: Dict, feature_embeddings: Dict) -> List[Tuple[str, float, List[Tuple[str, float]]]]:
    """ì¿¼ë¦¬ ë¬¸ì¥ì˜ ê° featureë³„ ì ìˆ˜ì™€ top 3 ì˜ˆì‹œ ë¬¸ì¥ ë° ìœ ì‚¬ë„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    # ì¿¼ë¦¬ ì„ë² ë”©
    query_embedding = get_embedding(query)
    if not query_embedding:
        return []
    
    feature_scores = []
    
    # ê° featureë³„ë¡œ ìœ ì‚¬ë„ê°€ ë†’ì€ ë¬¸ì¥ 3ê°œì˜ í‰ê·  ê³„ì‚° ë° ì˜ˆì‹œ ë°˜í™˜
    for feature, embeddings in feature_embeddings.items():
        similarities = []
        sentences = feature_sentences[feature]
        
        for i, embedding in enumerate(embeddings):
            similarity = cosine_similarity([query_embedding], [embedding])[0][0]
            similarities.append((sentences[i], similarity))
        
        # ìœ ì‚¬ë„ê°€ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ê³  ìƒìœ„ 3ê°œë§Œ ì„ íƒ
        similarities.sort(key=lambda x: x[1], reverse=True)
        top_3 = similarities[:3]
        avg_similarity = np.mean([sim for _, sim in top_3])
        feature_scores.append((feature, avg_similarity, top_3))
    
    # ìœ ì‚¬ë„ê°€ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬
    feature_scores.sort(key=lambda x: x[1], reverse=True)
    
    return feature_scores

def calculate_feature_sim_high_low(query: str, feature_sentences: Dict, feature_embeddings: Dict, n_avg: int = 5) -> Dict[str, Tuple[float, float]]:
    """
    ê° featureë³„ë¡œ ì¿¼ë¦¬ì™€ 'ë†’ë‹¤'/'ë‚®ë‹¤' ì˜ˆì‹œ ì„ë² ë”©ì˜ í‰ê· ê³¼ì˜ ìœ ì‚¬ë„(sim_high, sim_low)ë¥¼ ë°˜í™˜.
    n_avg: ìƒìœ„ nê°œ ì˜ˆì‹œì˜ í‰ê·  ìœ ì‚¬ë„ ì‚¬ìš©
    ë°˜í™˜: {feature: (sim_high, sim_low)}
    """
    query_embedding = get_embedding(query)
    if not query_embedding:
        return {}
    feature_sim = {}
    for feature in [
        'danceability', 'energy', 'loudness', 'speechiness',
        'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo']:
        # high
        high_key = f"{feature}_high"
        low_key = f"{feature}_low"
        if high_key not in feature_sentences or low_key not in feature_sentences:
            continue
        high_embeddings = feature_embeddings[high_key]
        low_embeddings = feature_embeddings[low_key]
        # ê° ì˜ˆì‹œ ì„ë² ë”©ê³¼ ì¿¼ë¦¬ ì„ë² ë”©ì˜ ìœ ì‚¬ë„ ê³„ì‚°
        sim_highs = [cosine_similarity([query_embedding], [emb])[0][0] for emb in high_embeddings]
        sim_lows = [cosine_similarity([query_embedding], [emb])[0][0] for emb in low_embeddings]
        # ìƒìœ„ nê°œ í‰ê· 
        sim_highs.sort(reverse=True)
        sim_lows.sort(reverse=True)
        sim_high = np.mean(sim_highs[:n_avg])
        sim_low = np.mean(sim_lows[:n_avg])
        feature_sim[feature] = (sim_high, sim_low)
    return feature_sim

def recommend_tracks(query: str, top_k: int = 20):
    feature_sentences, feature_embeddings = load_saved_data()
    feature_sim = calculate_feature_sim_high_low(query, feature_sentences, feature_embeddings, n_avg=5)
    feature_relevance = []
    for feature, (sim_high, sim_low) in feature_sim.items():
        if sim_high > sim_low:
            feature_relevance.append((feature, sim_high, 'high'))
        else:
            feature_relevance.append((feature, sim_low, 'low'))
    feature_relevance.sort(key=lambda x: x[1], reverse=True)
    top_features = feature_relevance[:3]

    # --- í„°ë¯¸ë„ì— ì¶œë ¥ ---
    print("\n[Featureë³„ ìœ ì‚¬ë„ (sim_high, sim_low)]")
    for feature, (sim_high, sim_low) in feature_sim.items():
        print(f"{feature}: sim_high={sim_high:.4f}, sim_low={sim_low:.4f}")

    print("\n[ìƒìœ„ 3ê°œ feature ë° ë°©í–¥ì„±]")
    for feature, relevance, direction in top_features:
        print(f"{feature}: {direction} (relevance={relevance:.4f})")
    # -------------------

    csv_path = os.path.join(data_dir, "spotify_tracknames_updated.csv")
    df = pd.read_csv(csv_path)

    def get_normalized_value(row, feature):
        value = row[feature]
        if pd.isnull(value):
            return 0
        if feature == 'loudness':
            return max(0, min(1, (float(value) + 45.92) / 46.672))
        elif feature == 'tempo':
            return max(0, min(1, float(value) / 232.198))
        else:
            return float(value)

    def calc_track_score(row):
        score = 0
        for feature, relevance, direction in top_features:
            value = get_normalized_value(row, feature)
            if direction == 'high':
                score += relevance * value
            else:
                score += relevance * (1 - value)
        return score

    df["recommend_score"] = df.apply(calc_track_score, axis=1)
    if 'language' in df.columns:
        df = df[df['language'].isin(['English', 'Korean'])]
    if 'popularity' in df.columns:
        df = df[df['popularity'] >= 20]
    df_sorted = df.sort_values("recommend_score", ascending=False)
    def get_title(row):
        return row["track_name"] if "track_name" in row and pd.notnull(row["track_name"]) else row["name"] if "name" in row and pd.notnull(row["name"]) else "Unknown"
    def get_artist(row):
        return row["artist_name"] if "artist_name" in row and pd.notnull(row["artist_name"]) else row["artists"] if "artists" in row and pd.notnull(row["artists"]) else "Unknown"
    df_top = df_sorted.head(500).copy()
    df_top['__title__'] = df_top.apply(get_title, axis=1)
    df_top = df_top.drop_duplicates(subset='__title__', keep='first')
    df_top['__artist__'] = df_top.apply(get_artist, axis=1)
    df_top = df_top.drop_duplicates(subset='__artist__', keep='first')
    top_n = df_top.head(top_k)

    results = []
    for _, row in top_n.iterrows():
        results.append({
            "track_name": get_title(row),
            "artist_name": get_artist(row),
            "track_uri": row.get("track_url", None),
            "recommend_score": row["recommend_score"],
            "language": row.get("language", None),
            "popularity": row.get("popularity", None)
        })
    return results

def main():
    # ì €ì¥ëœ ë°ì´í„° ë¡œë“œ
    print("Loading saved data...")
    feature_sentences, feature_embeddings = load_saved_data()
    
    # ì‚¬ìš©ì ì¿¼ë¦¬ ì…ë ¥
    query = input("\në¬¸ì¥ì„ ì…ë ¥í•˜ì„¸ìš”: ")
    
    # Featureë³„ sim_high, sim_low ê³„ì‚°
    print("\n=== Featureë³„ ì¿¼ë¦¬-ì˜ˆì‹œ ìœ ì‚¬ë„ (ë†’ë‹¤/ë‚®ë‹¤) ===")
    feature_sim = calculate_feature_sim_high_low(query, feature_sentences, feature_embeddings, n_avg=5)
    
    # --- input.txt ì €ì¥ìš© ì¶”ê°€ ì‹œì‘ ---
    output_lines: List[str] = []
    output_lines.append("\n=== Featureë³„ ì¿¼ë¦¬-ì˜ˆì‹œ ìœ ì‚¬ë„ (ë†’ë‹¤/ë‚®ë‹¤) ===")
    # --- input.txt ì €ì¥ìš© ì¶”ê°€ ë ---
    
    for feature, (sim_high, sim_low) in feature_sim.items():
        line = f"[{feature}] sim_high: {sim_high:.4f}, sim_low: {sim_low:.4f}"
        print(line)
        # --- input.txt ì €ì¥ìš© ì¶”ê°€ ---
        output_lines.append(line)
        # --------------------------------

    # === ìƒìœ„ 3ê°œ featureë§Œ ì‚¬ìš© ===
    feature_relevance = []
    for feature, (sim_high, sim_low) in feature_sim.items():
        if sim_high > sim_low:
            feature_relevance.append((feature, sim_high, 'high'))
        else:
            feature_relevance.append((feature, sim_low, 'low'))
    feature_relevance.sort(key=lambda x: x[1], reverse=True)
    top_features = feature_relevance[:3]
    print("\n=== ì¶”ì²œì— ì‚¬ìš©ëœ ìƒìœ„ 3ê°œ feature ë° ë°©í–¥ì„± ===")
    
    # --- input.txt ì €ì¥ìš© ì¶”ê°€ ---
    output_lines.append("\n=== ì¶”ì²œì— ì‚¬ìš©ëœ ìƒìœ„ 3ê°œ feature ë° ë°©í–¥ì„± ===")
    # --------------------------------
    
    for feature, relevance, direction in top_features:
        line = f"{feature}: {direction} (relevance={relevance:.4f})"
        print(line)
        # --- input.txt ì €ì¥ìš© ì¶”ê°€ ---
        output_lines.append(line)
        # --------------------------------

    # === ìŒì•… ì¶”ì²œ ===
    print("\n=== ì¿¼ë¦¬ ê¸°ë°˜ ìŒì•… ì¶”ì²œ Top 20 ===")
    
    # --- input.txt ì €ì¥ìš© ì¶”ê°€ ---
    output_lines.append("\n=== ì¿¼ë¦¬ ê¸°ë°˜ ìŒì•… ì¶”ì²œ Top 20 ===")
    # --------------------------------
    
    # 1. CSV ë¡œë“œ
    csv_path = os.path.join(data_dir, "spotify_tracknames_updated.csv")
    df = pd.read_csv(csv_path)

    def get_normalized_value(row, feature):
        value = row[feature]
        if pd.isnull(value):
            return 0
        if feature == 'loudness':
            return max(0, min(1, (float(value) + 45.92) / 46.672))
        elif feature == 'tempo':
            return max(0, min(1, float(value) / 232.198))
        else:
            return float(value)

    def calc_track_score(row):
        score = 0
        for feature, relevance, direction in top_features:
            value = get_normalized_value(row, feature)
            if direction == 'high':
                score += relevance * value
            else:
                score += relevance * (1 - value)
        return score

    df["recommend_score"] = df.apply(calc_track_score, axis=1)
    # ì–¸ì–´ê°€ ì˜ì–´(English) ë˜ëŠ” í•œêµ­ì–´(Korean)ì¸ ê³¡ë§Œ, popularity 20 ì´ìƒë§Œ í•„í„°ë§
    if 'language' in df.columns:
        df = df[df['language'].isin(['English', 'Korean'])]
    if 'popularity' in df.columns:
        df = df[df['popularity'] >= 20]
    df_sorted = df.sort_values("recommend_score", ascending=False)
    # ì¶©ë¶„íˆ ë§ì€ ê³¡(500ê°œ)ì—ì„œ ì œëª©+ì•„í‹°ìŠ¤íŠ¸ ê¸°ì¤€ ì¤‘ë³µ ì œê±° í›„ 20ê°œ ì¶”ì¶œ
    def get_title(row):
        return row["track_name"] if "track_name" in row and pd.notnull(row["track_name"]) else row["name"] if "name" in row and pd.notnull(row["name"]) else "Unknown"
    def get_artist(row):
        return row["artist_name"] if "artist_name" in row and pd.notnull(row["artist_name"]) else row["artists"] if "artists" in row and pd.notnull(row["artists"]) else "Unknown"
    df_top = df_sorted.head(500).copy()
    df_top['__title__'] = df_top.apply(get_title, axis=1)
    df_top = df_top.drop_duplicates(subset='__title__', keep='first')
    df_top['__artist__'] = df_top.apply(get_artist, axis=1)
    df_top = df_top.drop_duplicates(subset='__artist__', keep='first')
    top_20 = df_top.head(20)

    # 3. ì¶”ì²œ ê²°ê³¼ ì¶œë ¥ (ê³¡ëª…, ì•„í‹°ìŠ¤íŠ¸, ì ìˆ˜)
    for idx, row in top_20.iterrows():
        title = row["track_name"] if "track_name" in row else row["name"] if "name" in row else "Unknown"
        artist = row["artist_name"] if "artist_name" in row else row["artists"] if "artists" in row else "Unknown"
        score = row["recommend_score"]
        lang = row["language"] if "language" in row else "?"
        pop = row["popularity"] if "popularity" in row else "?"
        line = f"{idx+1}. {title} - {artist} (ì¶”ì²œ ì ìˆ˜: {score:.4f}, ì–¸ì–´: {lang}, popularity: {pop})"
        print(line)
        # --- input.txt ì €ì¥ìš© ì¶”ê°€ ---
        output_lines.append(line)
        # --------------------------------
    
    # --- input.txt ì €ì¥ìš© ì¶”ê°€: íŒŒì¼ë¡œ ì“°ê¸° ----------------------------
    txt_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), "input.txt")
    with open(txt_path, "w", encoding="utf-8") as f:
        f.write("\n".join(output_lines))
    print(f"\nğŸ“ ê²°ê³¼ë¥¼ '{txt_path}' ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
    # -----------------------------------------------------------------

if __name__ == "__main__":
    main() 
